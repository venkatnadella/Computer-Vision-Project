{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "# Assignment 1-3: Softmax\n",
    "\n",
    "This exercise is analogous to the SVM exercise. You will:\n",
    "\n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** with numerical gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR-10 Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "from data_utils import load_CIFAR10\n",
    "\n",
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the linear classifier. These are the same steps as we used for the\n",
    "    SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = r'C:\\Users\\Apple\\Downloads\\cifar-10-python\\cifar-10-batches-py'\n",
    "    \n",
    "    # Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n",
    "    try:\n",
    "       del X_train, y_train\n",
    "       del X_test, y_test\n",
    "       print('Clear previously loaded data.')\n",
    "    except:\n",
    "       pass\n",
    "\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "    \n",
    "    # subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "    \n",
    "    # Preprocessing: reshape the image data into rows\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "    \n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis = 0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    X_dev -= mean_image\n",
    "    \n",
    "    # add bias dimension and transform into columns\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n",
    "print('dev labels shape: ', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "Your need to complete `softmax_loss_naive`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First implement the naive softmax loss function with nested loops.\n",
    "# softmax_loss_naive function.\n",
    "\n",
    "from builtins import range\n",
    "import numpy as np\n",
    "\n",
    "def softmax_loss_naive(W, X, y, reg):\n",
    "    \"\"\"\n",
    "    Softmax loss function, naive implementation (with loops)\n",
    "\n",
    "    Inputs have dimension D, there are C classes, and we operate on minibatches\n",
    "    of N examples.\n",
    "\n",
    "    Inputs:\n",
    "    - W: A numpy array of shape (D, C) containing weights.\n",
    "    - X: A numpy array of shape (N, D) containing a minibatch of data.\n",
    "    - y: A numpy array of shape (N,) containing training labels; y[i] = c means\n",
    "      that X[i] has label c, where 0 <= c < C.\n",
    "    - reg: (float) regularization strength\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - loss as single float\n",
    "    - gradient with respect to weights W; an array of same shape as W\n",
    "    \"\"\"\n",
    "    # Initialize the loss and gradient to zero.\n",
    "    loss = 0.0\n",
    "    dW = np.zeros_like(W)\n",
    "\n",
    "    #############################################################################\n",
    "    # TODO: Compute the softmax loss and its gradient using explicit loops.     #\n",
    "    # Store the loss in loss and the gradient in dW. If you are not careful     #\n",
    "    # here, it is easy to run into numeric instability. Don't forget the        #\n",
    "    # regularization!                                                           #\n",
    "    #############################################################################\n",
    "    # *****START OF YOUR CODE*****\n",
    "    N, D=X.shape\n",
    "    C =W.shape[1]\n",
    "    #Loop \n",
    "    for i in range(N):\n",
    "        #Scores\n",
    "        scores=X[i].dot(W)\n",
    "        # Apply numeric stability trick by subtracting the maximum score\n",
    "        # from the scores to prevent large exponentials\n",
    "        shift_scores = scores - np.max(scores)\n",
    "\n",
    "        # Compute the softmax probabilities\n",
    "        softmax = np.exp(shift_scores) / np.sum(np.exp(shift_scores))\n",
    "\n",
    "        # Compute the loss for the current example\n",
    "        loss += -np.log(softmax[y[i]])\n",
    "\n",
    "        # Compute the gradient for each class\n",
    "        for j in range(C):\n",
    "            if j == y[i]:\n",
    "                dW[:, j] += (softmax[j] - 1) * X[i]\n",
    "            else:\n",
    "                dW[:, j] += softmax[j] * X[i]\n",
    "\n",
    "    # Average the loss and add regularization\n",
    "    loss /= N\n",
    "    loss += 0.5 * reg * np.sum(W * W)\n",
    "\n",
    "    # Average the gradient and add regularization\n",
    "    dW /= N\n",
    "    dW += reg * W\n",
    "\n",
    "\n",
    "    pass\n",
    "\n",
    "    # *****END OF YOUR CODE*****\n",
    "\n",
    "    return loss, dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.302374\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print('loss: %f' % loss)\n",
    "print('sanity check: %f' % (-np.log(0.1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: 0.867372 analytic: 0.867372, relative error: 4.928925e-08\n",
      "numerical: 1.739955 analytic: 1.739955, relative error: 2.385084e-09\n",
      "numerical: -0.803407 analytic: -0.803407, relative error: 2.506228e-08\n",
      "numerical: 1.213730 analytic: 1.213730, relative error: 3.486475e-08\n",
      "numerical: 0.818036 analytic: 0.818036, relative error: 2.983972e-08\n",
      "numerical: 1.710704 analytic: 1.710704, relative error: 1.442223e-08\n",
      "numerical: 0.939579 analytic: 0.939579, relative error: 5.660503e-08\n",
      "numerical: -1.364404 analytic: -1.364404, relative error: 1.976721e-09\n",
      "numerical: -1.946085 analytic: -1.946085, relative error: 7.385314e-09\n",
      "numerical: 0.193766 analytic: 0.193766, relative error: 2.761351e-07\n",
      "numerical: -1.865359 analytic: -1.865359, relative error: 1.505835e-08\n",
      "numerical: -0.661890 analytic: -0.661890, relative error: 8.096412e-08\n",
      "numerical: -0.366571 analytic: -0.366571, relative error: 2.059411e-08\n",
      "numerical: -5.213557 analytic: -5.213557, relative error: 2.258516e-09\n",
      "numerical: 1.501143 analytic: 1.501143, relative error: 4.022550e-08\n",
      "numerical: 0.066048 analytic: 0.066048, relative error: 7.268753e-07\n",
      "numerical: -0.637950 analytic: -0.637950, relative error: 3.321910e-09\n",
      "numerical: 1.379803 analytic: 1.379803, relative error: 3.050775e-08\n",
      "numerical: 2.019488 analytic: 2.019488, relative error: 1.118829e-08\n",
      "numerical: -0.179436 analytic: -0.179436, relative error: 1.568524e-07\n"
     ]
    }
   ],
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "from gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "\n",
    "# similar to SVM case, do another gradient check with regularization\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
    "# implement a vectorized version in softmax_loss_vectorized.\n",
    "\n",
    "def softmax_loss_vectorized(W, X, y, reg):\n",
    "    \"\"\"\n",
    "    Softmax loss function, vectorized version.\n",
    "\n",
    "    Inputs and outputs are the same as softmax_loss_naive.\n",
    "    \"\"\"\n",
    "    # Initialize the loss and gradient to zero.\n",
    "    loss = 0.0\n",
    "    dW = np.zeros_like(W)\n",
    "\n",
    "    #############################################################################\n",
    "    # TODO: Compute the softmax loss and its gradient using no explicit loops.  #\n",
    "    # Store the loss in loss and the gradient in dW. If you are not careful     #\n",
    "    # here, it is easy to run into numeric instability. Don't forget the        #\n",
    "    # regularization!                                                           #\n",
    "    #############################################################################\n",
    "    # *****START OF YOUR CODE*****\n",
    "# Number of training examples\n",
    "    num_train = X.shape[0]\n",
    "\n",
    "    # Compute the scores\n",
    "    scores = X.dot(W)\n",
    "\n",
    "    # Normalize the scores for numerical stability\n",
    "    scores -= np.max(scores, axis=1, keepdims=True)\n",
    "\n",
    "    # Compute the softmax probabilities\n",
    "    softmax_probs = np.exp(scores) / np.sum(np.exp(scores), axis=1, keepdims=True)\n",
    "\n",
    "    # Compute the loss\n",
    "    correct_class_probs = softmax_probs[np.arange(num_train), y]\n",
    "    loss = -np.sum(np.log(correct_class_probs))\n",
    "    loss /= num_train\n",
    "    loss += reg * np.sum(W * W)  # Regularization term\n",
    "\n",
    "    # Compute the gradient\n",
    "    softmax_probs[np.arange(num_train), y] -= 1  # Subtract 1 from the probabilities of the correct classes\n",
    "    dW = X.T.dot(softmax_probs)\n",
    "    dW /= num_train\n",
    "    dW += 2 * reg * W  # Regularization gradient\n",
    "    pass\n",
    "\n",
    "    # *****END OF YOUR CODE*****\n",
    "\n",
    "    return loss, dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive loss: 2.302374e+00 computed in 0.254773s\n",
      "vectorized loss: 2.302374e+00 computed in 0.064874s\n",
      "Loss difference: 0.000000\n",
      "Gradient difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# The two versions should compute the same results, but the vectorized version should be\n",
    "# much faster.\n",
    "\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
    "\n",
    "\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
    "\n",
    "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
    "# of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n",
    "print('Gradient difference: %f' % grad_difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Naive vs Vectorized Implementation\n",
    "Naive Implementation:\n",
    "\n",
    "The softmax_loss_naive function computes the softmax loss using nested loops.\n",
    "It iterates through each training example and class, computes scores, applies a stability trick, computes softmax probabilities, and then computes both the loss and gradients using explicit loops.\n",
    "This approach is intuitive but computationally inefficient due to nested loops.\n",
    "Vectorized Implementation:\n",
    "\n",
    "The softmax_loss_vectorized function computes the softmax loss and gradients using vectorized operations.\n",
    "It leverages matrix operations to compute scores, softmax probabilities, and gradients efficiently without explicit loops.\n",
    "This method is significantly faster for large datasets due to its use of optimized linear algebra operations.\n",
    "Performance Comparison\n",
    "\n",
    "Accuracy: Both implementations should produce identical results in terms of computed loss and gradients, as verified by comparing their outputs using the Frobenius norm for gradient differences. This ensures that the vectorized implementation does not sacrifice accuracy for speed.\n",
    "\n",
    "Numeric Gradient Checking\n",
    "Validation: To ensure correctness, numeric gradient checking (grad_check_sparse) is used to compare the analytic gradients computed by softmax_loss_naive against numerically computed gradients. This step verifies the correctness of your gradient implementation.\n",
    "Conclusion\n",
    "The Softmax classifier is a fundamental tool for multi-class classification tasks like CIFAR-10.\n",
    "Vectorization improves efficiency without sacrificing accuracy, making it suitable for handling large-scale datasets.\n",
    "Gradient checking provides additional validation, ensuring that the implemented gradients are correct.\n",
    "In summary,  implementation effectively demonstrates how to compute Softmax loss and gradients using both naive and vectorized approaches, highlighting the importance of vectorization for efficient computation in machine learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
